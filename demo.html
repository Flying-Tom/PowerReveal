<html><Head>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    
    <link rel="stylesheet" href="static/reveal.js/css/fonts/crmison.css" />
    <link rel="stylesheet" href="static/reveal.js/css/fonts/fira_code.css" />
    <link rel="stylesheet" href="static/reveal.js/css/fonts/ptsans.css" />
    <link rel="stylesheet" href="static/reveal.js/css/codehilite.css" />
    <link rel="stylesheet" href="static/reveal.js/css/reveal.css" />
    <link rel="stylesheet" href="static/reveal.js/css/reveal-slides.css" />
    
    <title>回归算法</title>
    <link rel="icon" href="static/img/flyingtom.jpeg" sizes="32x32">
    </Head>
    
    <body>
    
    <!-- <div id="logo" style="position: fixed; bottom: 20px; left: 20px; z-index: 5">
        <img height="48px" src ="static/img/nju-logo.png" />
    </div>
    <div id="logo" style="position: fixed; bottom: 20px; left: 90px; z-index: 5">
        <img height="48px" src ="static/img/njucs.jpg" />
    </div> -->
    
    <div class="reveal"><div class="slides">
    <section>
        <section><div class="slide-container">
            <div class="center middle">
                <h1 id="1">回归算法</h1>
                <div plugin="include(page='Slides_Author')">
                    <div class="hidden-in-outline author-block author-affiliation">
                    </div>
                    <div class="row hidden-in-outline author-block justify-content-md-center">
                        <div class="author-affiliation">
                            <a href="https://www.nju.edu.cn/">
                                <img alt="" class="inline-img" height="64px" src="static/img/nju-logo.png"></img>
                            </a>
                        </div>
                        &nbsp &nbsp
                        <div class="author-affiliation">
                            <a href="https://cs.nju.edu.cn/">
                                <img alt="" class="inline-img" height="64px" src="static/img/njucs.jpg"></img>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div></section>
    
        <section><div class="slide-container">
            <h2>Content</h2>
            <ul>
            <li>线性回归 ( Linear Regression )</li>
            <li>梯度下降 ( Gradient Descent )</li>
            <li>逻辑回归 ( Logisim Regression )</li>
            <li>正则化 ( Regularization )</li>
            </ul>
        </div></section>
    </section>
    
    <section>
        <section><div class="slide-container">
            <h2>什么是回归？</h2>
    
            <ul>
                <li>回归的目的是为了预测</li>
                <li>研究的是因变量（目标）和自变量（预测器）之间的关系</li>
                <li>用于预测分析，时间序列模型以及发现变量之间的因果关系</li>
                <li>通常使用曲线/线来拟合数据点,目标是使曲线到数据点的距离差异最小</li>
            </ul>
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>线性回归</h2>
            <p>
                线性回归假设目标值与特征之间线性相关，满足一个一次方程</p>
            <p>
                $$
                \hat{y}=\omega x+b
                $$
            </p>
            <p>
                $\hat{y}$为预测值，自变量x和因变量y是已知的，而我们想实现的是，对于一个新的$x$，能预测其对应的$y$
            </p>
            <p>以单变量线性回归为例，我们要找一条直线，并且让这条直线尽可能地拟合图中的数据点</p>
    
            <img height="300px" src="https://www.researchgate.net/profile/Hieu-Tran-17/publication/333457161/figure/fig3/AS:763959762247682@1559153609649/Linear-Regression-model-sample-illustration.ppm" >
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>如何建立模型？</h2>
            <p>以单变量线性回归为例，我们用以下的标记来描述回归问题:</p>
            <ul>
            <li>$m$ 代表训练集中实例的数量，也即训练样本的数量</li>
            <li>$x$ 代表特征/输入变量</li>
            <li>$y$ 代表目标变量/输出变量</li>
            <li>$\left( x,y \right)$ 代表训练集中的实例</li>
            <li>$({{x}^{(i)}},{{y}^{(i)}})$ 代表第$i$ 个观察实例</li>
            <li> $h$ 代表学习算法的解决方案或函数也称为假设（hypothesis） </li>
            </ul>
    
            <img height="300px" src="https://i.loli.net/2021/08/22/3WpeUa95A2Czoh8.png" >
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>多变量线性回归</h2>
            <p>
                $$
                \hat{y}=\omega x+b
                $$
                $$ x \Longrightarrow \left( {x_{1}},{x_{2}},...,{x_{n}} \right)$$
            </p>
            
                <table>
                    <thead>
                    <tr>
                    <th style="text-align:center">Size <br> (feet2)</th>
                    <th style="text-align:center">Number <br> of bedrooms</th>
                    <th style="text-align:center">Number of  <br> floors</th>
                    <th style="text-align:center">Age of <br> home <br> (years)</th>
                    <th style="text-align:center">Price <br> ($ 6000)</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                    <td style="text-align:center">2104</td>
                    <td style="text-align:center">5</td>
                    <td style="text-align:center">1</td>
                    <td style="text-align:center">45</td>
                    <td style="text-align:center">460</td>
                    </tr>
                    <tr>
                    <td style="text-align:center">1416</td>
                    <td style="text-align:center">3</td>
                    <td style="text-align:center">2</td>
                    <td style="text-align:center">40</td>
                    <td style="text-align:center">232</td>
                    </tr>
                    <tr>
                    <td style="text-align:center">1534</td>
                    <td style="text-align:center">3</td>
                    <td style="text-align:center">2</td>
                    <td style="text-align:center">30</td>
                    <td style="text-align:center">315</td>
                    </tr>
                    <tr>
                    <td style="text-align:center">852</td>
                    <td style="text-align:center">2</td>
                    <td style="text-align:center">1</td>
                    <td style="text-align:center">36</td>
                    <td style="text-align:center">178</td>
                    </tr>
                    <tr>
                    <td style="text-align:center">$\cdots$</td>
                    <td style="text-align:center">$\cdots$</td>
                    <td style="text-align:center">$\cdots$</td>
                    <td style="text-align:center">$\cdots$</td>
                    <td style="text-align:center">$\cdots$</td>
                    </tr>
                    </tbody>
                    </table>
                    $$
                    \hat{y} =\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}
                    $$ 
        </div></section>
    
        <section><div class="slide-container">
            <h2>多变量线性回归</h2>
            <p>增添更多特征后，我们引入一系列新的注释：</p>
            <ul>
                <li>$n$ 代表特征的数量</li>
                <li>${x^{\left( i \right)}}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个向量</li>
                比方说，上图的${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$
                <li>${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征</li>
                如上图的$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$，
            </ul>
        </div></section>
    
        <section><div class="slide-container">
            <h2>多变量线性回归</h2>
            <p>
                支持多变量的假设 $h$ 表示为：
    
                $$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$$
    
                这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：
    
                $$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$$
                
                此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m \times (n+1)$。因此最终公式可以简化为：
                
                $$h_{\theta} \left( x \right)={\theta^{T}}X$$
                
                其中上标$T$代表矩阵转置
            </p>
    
        </div></section>
    
    </section>
    
    
    
    <section>
        <section><div class="slide-container">
            <h2> 如何评估模型？ </h2>
            <p>给定数据集，假如我们已经训练出了一个模型：</p>
    
            <br>
            我们应该如何衡量这个模型和数据集之间的拟合程度呢?
    
            <br><br>
    
            <div class="center">
                <img src="https://miro.medium.com/max/2000/1*1Ba62SxY84fIxF6U-FiZ-g.png"> </img>
                <br>
                <p style="font-size: 20px;"> Different models perform differently </p>
            </div>
    
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>代价函数</h2>
            <p>以单变量线性回归为例</p>
    
            <p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，
                模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。</p>
            
            <img src="https://i.loli.net/2021/08/22/zZrhtDQ4KAdBVbk.png"> </img>
    
            <p>我们的目标便是选择出可以使得建模误差最小的模型参数。
            
            为了量化建模误差，我们引入代价函数（Cost Function）
    
            $$J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$$
    
            </p>
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>代价函数</h2>
            
            <p>
                我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：
                <br>
                <div class="center">
                    <img src="https://i.loli.net/2021/08/22/Xh3OApqnvVWf6St.png"> </img>
                </div>
                <br>
            </p>
            <p>
                则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点
                <br><br>
                而代价函数表示的是这个模型拟合的有多差，因而求解回归问题,最优化模型，也就转化为了对代价函数的最小化问题
            </p>
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>代价函数</h2>
    
            <p>$$J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$$</p>
            <br>
            <p>
                上述的这个代价函数也被称作平方误差函数，有时也被称为均方误差代价函数
    
                <br><br><br>
                我们之所以要求出误差的平方和，是因为均方误差代价函数对于大多数问题，特别是回归问题，都是一个合理的选择
    
                <br><br>
                代价函数本质上是量化建模误差，因而还有其他的代价函数
            </p>
    
            
        </div></section>
    
        <section>
            <h2>代价函数</h2>
            <br>
            <p>我们可以将单变量的线性回归问题推广到多变量线性回归</p>
            <br><br>
            <table><tr>
                <td><img src="https://i.loli.net/2021/08/16/WJxl7NHaFKzkMj6.png" border=0></td>
                <td>$\Longrightarrow \qquad$</td>
                <td><img src="https://i.loli.net/2021/08/16/u1TRve36SZr8zJF.png" border=0></td>
            </tr></table>
            <br><br>
            <p>新问题: 如何求代价函数的最小值？</p>
        </section>
    </section>
    
    
    <section>
        <section><div class="slide-container">
            <h2>如何求代价函数的最小值？</h2>
            <br>
            <p> 在山上的人要怎样最快的下山？ </p>
            <br>
        
            <p class="fragment" > 一个符合常识的方法： 每一步都走最陡峭的地方 </p>
        
            <br><br><br><br><br>
        
            <div class="center">
                <img src="https://editor.analyticsvidhya.com/uploads/70205gd%20mountain.jpg"> </img>
                <br>
                <p style="font-size: 20px;"> Finding the lowest point in a hilly landscape. </p>
            </div>
        
        </div></section>
    
        <section><div class="slide-container">
            <h2>梯度下降算法</h2>
            <p> 梯度下降的基本过程就和下山的场景很类似。
    
                <ul>
                <li>首先，我们有一个可微分的函数 ( 山 )</li>
                <li>我们的目标就是找到这个函数的最小值 ( 山底 ) </li>
                <li>而梯度的反方向则是函数值局部下降最快的方向 ( 最陡峭 ) </li>
                <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent">Why the gradient is the direction of steepest ascent ?</a>
                </ul>
            </p>
            <br>
            <p class="fragment">
                梯度下降算法的具体思路是：
                <br>
                首先随机选择一个参数组合
                $\left( {\theta_{0}},{\theta_{1}},......,{\theta_{n}} \right)$，
                计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合
                <br>
                重复直到找到一个局部最小值（Local Minimum）
            </p>
    
            <p class="fragment">
                但因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（Global Minimum），
                选择不同的初始参数组合，可能会找到不同的局部最小值
                <br>
                所以很多时候，梯度下降找到的并不是最小值，而是极小值或是鞍点
            </p>
        
        </div></section>
    
        <section><div class="slide-container">
            <h2>梯度下降算法</h2>
            <p> 局部最小值</p>
            
            <br><br>
        
            <div class="center">
                <img height="300px" src="https://i.loli.net/2021/08/16/Eb4DahVU1QYRZMo.png"> </img>
                <br>
                <p style="font-size: 20px;"> Starting at different points leads to different local minimum. </p>
            </div>
            <br><br>
            <p> $\star$ 模拟退火</p>
        </div></section>
    
    
        <section><div class="slide-container">
            <h2>梯度下降算法</h2>
            <p> 
                算法具体表示如下：
    
                $$ \begin{aligned}
                &\text { repeat until convergence }\{\\
                &\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right) \quad(\text { for } j=0 \text { and } j=1)\\
                &\}
                \end{aligned}
                $$
            </p>
            <p>
                其中:
            
                <ul>
                    <li>$a$是学习率 ( Learning Rate )，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大</li>
                    <li>$J(\theta)$ 是代价函数</li>
                </ul>
            </p>
            <br>
            <p class=>对$\theta $赋值，使得$J\left( \theta  \right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值</p>
            
        
        </div></section>
    
        <section><div class="slide-container">
            <h2>梯度下降算法</h2>
            <p> 更直观的理解 </p>
            <div>$$\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta\right) $$</div>
            <div>$$\theta:=\theta -\alpha \nabla J\left(\theta\right) $$</div>
            <br><br>
            <img height="300px" style="margin-left: 120px;" src="https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png"> </img>
                
        
        </div></section>
    
        <section><div class="slide-container">
            <h2>梯度下降算法</h2>
            <ul>
                学习率$a$的选择
                <li>
                    如果$a​$太小，学习速率太小，需要很多步才能到达最低点
                </li>
                <li>
                    如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛
                </li>
            </ul>
            <br><br>
            <img height="300px" src="https://i.loli.net/2021/08/16/fwkF8epd6izr47V.png"> </img>
    
        
        </div></section>
    
    </section>
    
    <section>
        <section><div class="slide-container">
            <h2>分类问题</h2>
    
            <ul>
                宽泛意义上的分类问题
                <li>用户特征分类</li>
                <li>疾病检测</li>
                <li>图像识别</li>
                <li>$\cdots$</li>
                
            </ul>
            <br><br>
            <ul>
                “是” 或 “不是”
                <li>购买商品</li>
                <li>垃圾邮件</li>
                <li>恶意程序</li>
                <li>金融交易</li>
    
            </ul>
        </div></section>
    
        <section><div class="slide-container">
            <h2>二分类问题</h2>
            <p>$\quad$</p>
            <p> $\blacktriangledown$ 定义：</p>
            <p style="text-indent: 2em;">
                二分类问题就是给定输入$x$，判断它的标签是0类还是1类,也就是0-1分类问题
            </p>
            <br>
            最基本的分类问题
    
            $$ \text{多分类问题} \xrightarrow[\text{分别判断输入是否属于某个类}]{\operatorname{One-Vs-All}} \text{二分类问题} $$
    
            <img src="https://i.loli.net/2021/08/22/2thycPOfYVTS6jJ.png" height="300px"> </img>
            
        </div></section>
        <section><div class="slide-container">
            <h2>二分类问题</h2>
            <p>$\quad$</p>
    
            <div>
    
            <p> 问题的解是标签值！ </p>
            <br><br>
            <p> 如何用连续的数值去预测离散的标签值呢？ </p>
            <ul>
                <li> 设定一个阈值  $\longrightarrow$ <b> 感知机 </b> ( Perceptron ) </li>
                <li> $[0,1]$ 区间预测标签的概率 $\longrightarrow$ <b> 逻辑回归 </b> ( Logistics Regression )</li>
            </ul> 
            </div>
        </div></section>
    </section>
    
    <section>
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> $\blacktriangledown$ 假设函数：</p>
            <p style="text-indent: 2em;">
                逻辑回归模型的假设函数是： $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$
                <br><br>
                其中：
                <ul>
                    <li>$X$ 代表特征向量</li>
                    <li>$g$ 代表逻辑函数（Logistic function)</li>
                </ul>
    
                
    
                $$ g(z) = \frac{1}{1+e^{-z}}$$
            </p>
    
            <div class="center">
                <img src="https://pic2.zhimg.com/80/v2-1562a80cf766ecfe77155fa84931e745_720w.png"> </img>
                <br>
                <p style="font-size: 18px;"> Sigmoid函数 </p>
            </div>
    
    
            
        </div></section>
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> $\blacktriangledown$ 假设函数：</p>
            <p style="text-indent: 2em;">
                假设函数$h(\theta)$表示样本被预测为正例1的概率,所以
    
                $$P(y = 1 | x ; \theta ) = h_{\theta}(x)$$
    
                $$P(y = 0 | x ; \theta ) = 1 - h_{\theta}(x)$$
            </p>
    
            <p>
                上述两式可以合并为一个式子
    
                $$ P(y|x;\theta) = (h_{\theta}(x))^{y} ( 1 - h_{\theta}(x))^{1-y}$$
            </p>
    
            <p>
                由上式得似然函数:
    
                $$
                L(\theta)=\prod_{i=1}^{m} P\left(y^{(i)} \mid x^{(i)} ; \theta\right)=\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
                $$
            </p>
    
    
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> 似然函数 $\longrightarrow$ 对数似然函数 </p>
            <p>
                $$
                l(\theta)=\log L(\theta)=\sum_{i=1}^{m}\left(y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)
                $$
    
                当（对数）似然函数求得最大值时，模型能够最大可能的满足当前的样本
    
                <br><br><br>
            </p>
    
            <p class="fragment">
                那么我们令：
    
                $$
                J(\theta)=-\frac{1}{m} l(\theta)
                $$
    
                $$ \text{最大化} \Longrightarrow \text{最小化} $$
            </p>
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> 所以代价函数为 </p>
            <p>
                
                $$J(\theta) = -\frac{1}{m} l(\theta)$$ 
                $$= -\frac{1}{m} \sum_{i=1}^{n}\left(y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)$$
                $$= \frac{1}{m} \sum_{i=1}^m \operatorname{Cost}(h_{\theta}(x^{(i)}),y^{(i)})$$
            </p>
            <p>
                其中:
                $$
                \operatorname{Cost}\left(h_{\theta}(x), y\right)= \begin{cases}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\ -\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{cases}
                $$
            </p>
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> Q: 为什么不用平方和(MSE)代价函数？ </p>
            <p style="text-decoration: line-through;font-size: 24px;color: rgba(100, 99, 99, 0.781);">
                我们之所以要求出误差的平方和，是因为误差平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择
            </p>
            <br><br>
            <div class="fragment">
                <img class="center" src="https://i.loli.net/2021/08/22/XljdOyKuJxPLcfN.jpg"> </img>
                <br>
    
                <p> 代入MSE代价函数，得到的代价函数是非凸的（non-convexfunction）</p>
                <br>
                <p class="fragment"> 而对数形式的代价函数 ( 交叉熵代价函数 ) 是一个凸函数, 并且没有局部最优值</p>
            </div>
    
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> Q: 为什么不用平方和(MSE)代价函数？ </p>
            <p >如果使用MSE作为代价函数，对于单变量逻辑回归</p>
            <ul> 
                <li>预测值$\hat{y} = \sigma(\omega \cdot x + b)$($\sigma$是sigmoid函数)</li>
                <li> 代价函数为 $C = \frac{(y - \hat{y})^2}{2}$</li>
            </ul>
    
            <p>我们采用梯度下降的方式对参数$\omega,b$进行更新，也即分别将损失函数对这两个参数求导</p>
            
            $$
            \begin{aligned}
            &\frac{\partial C}{\partial w}=(\hat{y}-y) \sigma^{\prime}(z) x=(\hat{y}-y) \sigma^{\prime}(z) x \\
            &\frac{\partial C}{\partial b}=(\hat{y}-y) \sigma^{\prime}(z) x=(\hat{y}-y) \sigma^{\prime}(z)
            \end{aligned}
            $$
    
            <p>可以看到 $\omega, b$ 的更新速率与当前的预测值sigmoid函数的导数有关</p>
            
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> Q: 为什么不用平方和(MSE)代价函数？ </p>
            <p >sigmoid函数的图像如下</p>
    
            <img  src="https://pic2.zhimg.com/80/v2-1562a80cf766ecfe77155fa84931e745_720w.png" width="400px" >
    
            <p>所以，如果当前模型的输出接近0或者1时， $\sigma^{\prime}(z)$ 就会非常接近0，使得求得的梯度很小，损失函数收敛的很慢</p>
            
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p style="font-size: 24px;">
                利用梯度下降法求解$\theta$
            </p>
            <p style="font-size: large;">
            $$J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]$$ 
    
            考虑: $h_{\theta}\left(x^{(i)}\right)=\frac{1}{1+e^{-\theta} T_{x^{(i)}}}$ 则:
    
            $$ y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)$$
            \begin{align}
            &=y^{(i)} \log \left(\frac{1}{1+e^{-\theta} T_{z^{(i)}}}\right)+\left(1-y^{(i)}\right) \log \left(1-\frac{1}{1+e^{-\theta} T_{x^{(i)}}}\right)\\
            &=-y^{(i)} \log \left(1+e^{-\theta^{T} x^{(i)}}\right)-\left(1-y^{(i)}\right) \log \left(1+e^{\theta^{T} x^{(i)}}\right)
            \end{align}
    
            所以: 
        
            \begin{align}
            \frac{\partial}{\partial \theta_{j}} J(\theta)
            &=\frac{\partial}{\partial \theta_{j}}\left[-\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(1+e^{-\theta^{T} x^{(i)}}\right)-\left(1-y^{(i)}\right) \log \left(1+e^{\theta^{T} x^{(i)}}\right)\right]\right.\\
            &=-\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-\frac{1}{1+e^{-\theta} T_{x^{(i)}}(i)}\right) x_{j}^{(i)}\\
            &=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right] x_{j}^{(i)}\\
            &=\frac{1}{m} \sum_{i=1}^{m}\left[h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right] x_{j}^{(i)}\\
            \end{align}
            <p>
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> 我们得到
    
                $$
                    \frac{\partial}{\partial \theta_{j}} J(\theta) = \frac{1}{m} \sum_{i=1}^{m}\left[h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right] x_{j}^{(i)}
                $$
    
            </p>
            
            <br><br>
    
            <p>梯度下降更新式
                $$
                \theta_{j}=\theta_{j} - \alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
                $$
            </p>
            <p >
            所以$\theta$最终的更新式为：
    
            $$
            \theta_{j}:=\theta_{j}- \alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
            $$
        </p>
        <p >
        所以$\theta$最终的更新式为：

        $$
        \theta_{j}:=\theta_{j}- \alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
        $$

        <p>
    </div></section>

    <section><div class="slide-container">
        <h2>逻辑回归</h2>
        <p>$\quad$</p>
        <p> 逻辑回归与线性回归的区别与联系 </p>

        <ul>
            区别
            <li>线性回归假设响应变量服从正态分布，逻辑回归假设响应变量服从伯努利分布</li>
            <li>线性回归优化的目标函数是均方差（最小二乘），而逻辑回归优化的是似然函数（交叉熵）</li>
            <li>线性回归分析的是因变量自身与自变量的关系，而逻辑回归研究的是因变量取值的概率与自变量的概率</li>
            <li>逻辑回归处理的是分类问题，线性回归处理的是回归问题，这也导致了两个模型的取值范围不同：0-1和实数域</li>
        </ul>

    </div></section>

    <section><div class="slide-container">
        <h2>逻辑回归</h2>
        <p>$\quad$</p>
        <p> 逻辑回归与线性回归的区别与联系 </p>

        <ul>
            联系
            <li>两个都是线性模型，线性回归是普通线性模型，逻辑回归是广义线性模型</li>
            <li>表达形式上，逻辑回归是线性回归套上了一个Sigmoid函数</li>
        </ul>

    </div></section>
</section>


<section>
    <section><div class="slide-container">
        <h2>过拟合</h2>

        <img src="https://i.loli.net/2021/08/24/IuorOhTC68fbGwL.jpg" >

        <p>训练集表现很好，测试集表现较差</p>

        <br><br>

        <ul class="fragment">
            <li>欠拟合：啥都没看，挂了</li>
            <li>理想拟合：看了例题，刷了样卷掌握方法，考试正常发挥</li>
            <li>过拟合：看了例题，样卷刷出来张张满分，但只会做样卷，挂了</li>
        </ul>

    </div></section>

    <section><div class="slide-container">
        <h2>过拟合</h2>

        <img src="https://i.loli.net/2021/08/24/tRQ57n2PdzALJmg.jpg" >
        <br>

        <p>我们从多项式的角度理解</p>
        <ul>
            <li>$x$ 的次数越高 ，拟合效果越好 </li>
            <li>泛化能力变弱，相应的预测的能力就可能变差</li>
        </ul>
        <br><br>
        <p>如何解决正则化问题？</p>
        <ul>
            <li>丢弃一些不能帮助我们正确预测的特征</li>
            <li>保留所有的特征，但是减少参数的大小, 惩罚高阶项 $\quad$（正则化）</li>
        </ul>
    </div></section>

    <section><div class="slide-container">
        <h2>过拟合</h2>
        <p>高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了</p>

        <div class="fragment">
            <p>对于上面的问题，我们令其模型为</p>
            <ul>
                区别
                <li>线性回归假设响应变量服从正态分布，逻辑回归假设响应变量服从伯努利分布</li>
                <li>线性回归优化的目标函数是均方差（最小二乘），而逻辑回归优化的是似然函数（交叉熵）</li>
                <li>线性回归分析的是因变量自身与自变量的关系，而逻辑回归研究的是因变量取值的概率与自变量的概率</li>
                <li>逻辑回归处理的是分类问题，线性回归处理的是回归问题，这也导致了两个模型的取值范围不同：0-1和实数域</li>
            </ul>
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>逻辑回归</h2>
            <p>$\quad$</p>
            <p> 逻辑回归与线性回归的区别与联系 </p>
    
            <ul>
                联系
                <li>两个都是线性模型，线性回归是普通线性模型，逻辑回归是广义线性模型</li>
                <li>表达形式上，逻辑回归是线性回归套上了一个Sigmoid函数</li>
            </ul>
    
        </div></section>
    </section>
    
    
    <section>
        <section><div class="slide-container">
            <h2>过拟合</h2>
    
            <img src="https://i.loli.net/2021/08/24/IuorOhTC68fbGwL.jpg" >
    
            <p>训练集表现很好，测试集表现较差</p>
    
            <br><br>
    
            <ul class="fragment">
                <li>欠拟合：啥都没看，挂了</li>
                <li>理想拟合：看了例题，刷了样卷掌握方法，考试正常发挥</li>
                <li>过拟合：看了例题，样卷刷出来张张满分，但只会做样卷，挂了</li>
            </ul>
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>过拟合</h2>
    
            <img src="https://i.loli.net/2021/08/24/tRQ57n2PdzALJmg.jpg" >
            <br>
    
            <p>我们从多项式的角度理解</p>
            <ul>
                <li>$x$ 的次数越高 ，拟合效果越好 </li>
                <li>泛化能力变弱，相应的预测的能力就可能变差</li>
            </ul>
            <br><br>
            <p>如何解决过拟合问题？</p>
            <ul>
                <li>丢弃一些不能帮助我们正确预测的特征 $\quad$（特征选择） </li>
                <li>保留所有的特征，但是减少参数的大小, 惩罚高阶项 $\quad$（正则化）</li>
            </ul>
        </div></section>
    
        <section><div class="slide-container">
            <h2>过拟合</h2>
            <p>高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了</p>
    
            <div class="fragment">
                <p>对于上面的问题，我们令其模型为</p>
                <ul>
                    <li>$h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}^{2}+\theta_{3} x_{3}^{3}+\theta_{4} x_{4}^{4}$</li>
                    <li>$ J(\theta) = \frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \right]$</li>
                </ul>
            <p>我们要减小$\theta_3,\theta_4$ , 为它们设置一点惩罚，将代价函数改为</p>
    
            $$J(\theta) = \frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+1000 \theta_{3}^{2}+1000 \theta_{4}^{2}\right]$$
    
            <img height="200px" src="https://i.loli.net/2021/08/24/uUxHczsKyPY9tE5.jpg">
            </div>
    
        </div></section>
    </section>
    
    
    <section>
        <section><div class="slide-container">
            <h2>正则化</h2>
            <p>定义: 修改学习算法使其降低泛化误差而非训练误差</p>
    
            <p>常见的策略：
                <ul>
                    <li>向机器学习模型添加限制参数值的额外约束 </li>
                    <li>向目标函数增加额外项来对参数值进行软约束</li>
                </ul>
    
            </p>
    
            <p>
                I. 参数范数惩罚
                <ul>
                    <li>$L^2$参数正则化</li>
                    <li>$L^1$参数正则化</li>
                </ul>
                <p>
                    $$
                    \tilde{J}({\theta} ; {X}, {y})=J({\theta} ; {X}, {y})+ \alpha \Omega({\theta})
                    $$
                    其中 $\alpha \in[0, \infty)$ 是权衡范数惩罚项 $\Omega$ 和标准目标函数 $J({X} ; {\theta})$ 相对贡献的超参数, 
                    将 $\alpha$ 设为 0 表示没有正则化,
                    $\alpha$ 越大，对应正则化惩罚越大
                </p>
            </p>
            <br>
            <p>II. 作为约束的范数惩罚</p>
    
            <br>
    
        </div></section>
    
    
        <section><div class="slide-container">
            <h2>参数范数惩罚</h2>
    
            <p>
                范数是衡量向量的概念，向量的$L^p$范数定义为：
                $$\|\mathbf{x}\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}, p \in \mathbb{R}, p \geq 1$$
            </p>
    
    
            <p>
                $L^2$范数 （欧几里得范数），是向量到原点的欧几里得距离
    
                $$\|\mathbf{x}\|_{2}=\sqrt{\sum_{i}|x|^{2}}$$
            </p>
            <p> 我们也经常用$L^2$范数的平⽅来衡量向量
      
                $$\|\mathbf{x}\|_{2}^{2}=\sum_{i}|x|^{2} = x^T x$$
    
                后者计算上更为便利，例如它的对 $\mathbf{x}$ 梯度的各个分量只依赖于 $\mathbf{x}$ 的对应的各个分量，而 $L^2$
                范数对 $\mathbf{x}$ 梯度的各个分量要依赖于整个 $\mathbf{x}$ 向量
            </p>
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>$L^2$参数正则化</h2>
    
            <p>
                $L^{2}$ 通常被称为权重衰减 （ weight decay ），它通过向目标函数添加一个正则项 $\Omega({\theta})=\frac{1}{2}\|{w}\|_{2}^{2}$, 使权重更加接近原点 ( 岭回归或 Tikhonov 正则 )
            </p>
    
            <p>
                $$ \tilde{J}({w} ; {X}, {y})=  J({w} ; {X}, {y})  +  \frac{\alpha}{2} {w}^{T} {w}$$
            </p>
    
            <p>例如：正则化线性回归的代价函数
            $$
            J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left[\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]
            $$
            其中，$\lambda$ 是正则化参数（Regularization Parameter）
            </p>
    
            <ul>
                <li> $\lambda$ 过小 $\longrightarrow$ 过拟合 </li>
                <li> $\lambda$ 过大 $\longrightarrow$ 欠拟合 </li>
            </ul>
        </div></section>
    
        <section><div class="slide-container">
            <h2>参数范数惩罚</h2>
            <p> $L^{2}$ 范数非常常见于正则化，但是并不一定适用于所有的情况</p>
            <p>比如它在原点附近的增长十分缓慢, 因此不适用于需要区别 0 和非常小但是非 0 值的情况</p>
            <img height="280px" src="https://miro.medium.com/max/1400/1*bM2txQ6caL4AKiN19oH5bQ.gif" >
            <p class="fragment">
                此时，$L^{1}$ 范数就是一个比较好的选择，它在所有方向上的增长速率都是一样的，定义为:
                $$
                \|\mathbf{x}\|_{1}=\sum_{i}\left|x_{i}\right|
                $$
                它经常使用在需要区分 <b>0 和非 0</b> 元素的情形
            </p>
    
        </div></section>
    
        <section><div class="slide-container">
            <h2>$L^1$参数正则化</h2>
    
            <p>
                类似于$L^2$正则化：
            </p>
    
            <p>
                $$
                \tilde{J}({w} ; {X}, {y})= J({w} ; {X}, {y}) + \alpha\|{w}\|_{1}
                $$
            </p>
    
            <p>
                $L^1$正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
            </p>
            <p>
                稀疏矩阵指的是很多元素为0、只有少数元素是非零值的矩阵。以线性回归为例，即得到的线性回归模型的大部分系数都是0，这表示只有少数特征对这个模型有贡献，从而实现了特征选择。
            </p>
        </div></section>
    
    
        <section><div class="slide-container">
            <h2>$L^1$参数正则化</h2>
    
            <p>
                为什么L1正则化能产生稀疏模型？
            </p>
    
            <p>假设只考虑二维的情况，即只有两个权值 $\omega^{1}$ 和 $\omega^{2}$, 此时的$L^1$正则化公式即为: $L^1=\left|\omega^{1}\right|+ \left| \omega^{2}\right| $</p>
                
            <p>对代价函数使用梯度下降法求解, 则求解 $J$ 的过程可以画出等值线，同时$L^1$正则化的函数也可以在二维平面上画出来, 首次相交的地方即为最优解</p>
    
            <img src="https://i.loli.net/2018/11/28/5bfe89e366bba.jpg" alt="">
        
        </div></section>
        
    </section>
    
    <section>
        <section><div class="slide-container">
            <h2>Acknowledgement</h2>
    
            <ul>
                <li> Andrew Ng - Machine Learning </li>
                <li> Deep Learning </li>
                <li> CSDN</li>
                <li> 知乎</li>
            </ul>
        </div></section>
    </section>
    
    </div></div>
    
    
    <script src="static/reveal.js/js/reveal.js"></script>
    <script src="static/reveal.js/js/initialize.js"></script>
    
    <script>
        MathJax = {
            tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
            svg: {fontCache: 'global'}
        };
    </script>
    <script id="MathJax-script" async src="static/reveal.js/js/tex-svg.js"></script>
    
    </body>
    </html>